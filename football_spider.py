import requests, pymysql, re, time
from bs4 import BeautifulSoup

# === é…ç½® ===
DB_PASSWORD = "20041217"
DB_CONFIG = {"host": "localhost", "user": "root", "password": DB_PASSWORD, "db": "football_data", "charset": "utf8mb4"}
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}

LEAGUE_NAMES = {24646: "è‹±è¶…", 24651: "è¥¿ç”²", 24596: "æ„ç”²", 24648: "å¾·ç”²", 24652: "æ³•ç”²"}


def get_conn():
    return pymysql.connect(host=DB_CONFIG["host"], user=DB_CONFIG["user"], password=DB_CONFIG["password"],
                           database=DB_CONFIG["db"], cursorclass=pymysql.cursors.DictCursor)


def fetch(url, params=None):
    try:
        if __name__ == "__main__": time.sleep(0.3)
        res = requests.get(url, headers=HEADERS, params=params, timeout=10)
        res.raise_for_status()
        res.encoding = 'utf-8'
        return res
    except:
        return None


def clean_text(text): return text.replace(" ", "").strip() if text else ""


# === æ ¸å¿ƒåŠŸèƒ½ 1ï¼šåˆ·æ–°è”èµ›æ¦œå• (ç§¯åˆ†/å°„æ‰‹/åŠ©æ”») ===
def update_league_data(lid):
    lname = LEAGUE_NAMES.get(lid, "æœªçŸ¥è”èµ›")
    print(f"ğŸ”„ [è‡ªåŠ¨çˆ¬è™«] æ­£åœ¨åˆ·æ–° {lname} æ¦œå•æ•°æ®...")
    conn = get_conn()
    c = conn.cursor()

    # 1. ç§¯åˆ†æ¦œ
    res = fetch("https://www.dongqiudi.com/sport-data/soccer/biz/data/standing",
                {"season_id": lid, "app": "dqd", "version": "0", "platform": "web"})
    if res:
        try:
            items = res.json()['content']['rounds'][0]['content']['data']
            for i in items:
                c.execute("REPLACE INTO standings VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)",
                          (lid, i['team_id'], i['rank'], i['matches_total'], i['matches_won'], i['matches_draw'],
                           i['matches_lost'], i['goals_pro'], i['goals_against'], i['points']))
        except:
            pass

    # 2. å°„æ‰‹/åŠ©æ”»æ¦œ
    for t in ['goals', 'assists']:
        res = fetch("https://www.dongqiudi.com/sport-data/soccer/biz/data/person_ranking",
                    {"season_id": lid, "type": t, "app": "dqd", "version": "0", "platform": "web"})
        if res:
            try:
                c.execute("DELETE FROM rankings WHERE league_id=%s AND type=%s", (lid, t))
                data = [
                    (lid, t, p['rank'], p['person_id'], p['person_name'], p['team_name'], p.get('goal', p.get('count')))
                    for p in res.json()['content']['data']]
                c.executemany(
                    "INSERT INTO rankings (league_id, type, `rank`, person_id, name, team, count) VALUES (%s,%s,%s,%s,%s,%s,%s)",
                    data)
            except:
                pass

    conn.commit()
    conn.close()
    return True


# === æ ¸å¿ƒåŠŸèƒ½ 2ï¼šåˆ·æ–°å•ä¸ªçƒé˜Ÿ (èµ„æ–™+é˜µå®¹) ===
def update_team_data(tid):
    print(f"ğŸ”„ [è‡ªåŠ¨çˆ¬è™«] æ­£åœ¨åˆ·æ–°çƒé˜Ÿ: {tid} ...")
    conn = get_conn()
    c = conn.cursor()

    # å…ˆæŸ¥ä¸€ä¸‹ league_idï¼Œé˜²æ­¢è¦†ç›–æ—¶ä¸¢å¤±
    c.execute("SELECT league_id FROM teams WHERE team_id = %s", (tid,))
    res = c.fetchone()
    lid = res['league_id'] if res else 0

    res = fetch(f"https://www.dongqiudi.com/team/{tid}.html")
    if not res:
        conn.close()
        return False

    soup = BeautifulSoup(res.text, 'html.parser')

    # è§£æçƒé˜Ÿä¿¡æ¯
    info = {"name_cn": "", "name_en": "", "founded": "", "country": "", "city": "", "stadium": "", "capacity": "",
            "phone": "", "email": "", "address": "", "logo": ""}
    con = soup.find('div', class_='info-con')
    if con:
        if con.find('p', class_='team-name'): info['name_cn'] = con.find('p', class_='team-name').get_text(strip=True)
        if con.find('p', class_='en-name'): info['name_en'] = con.find('p', class_='en-name').get_text(strip=True)
        map_ = {"æˆç«‹": "founded", "å›½å®¶": "country", "åŸå¸‚": "city", "ä¸»åœº": "stadium", "å®¹çº³": "capacity",
                "ç”µè¯": "phone", "é‚®ç®±": "email", "åœ°å€": "address"}
        for tag in con.find_all(['span', 'p']):
            txt = tag.get_text(" ", strip=True)
            for k, v in map_.items():
                if k in clean_text(txt): info[v] = re.split(r'[:ï¼š]', txt)[-1].strip()

    img = soup.find('div', class_='team-info').find('img', class_='team-logo')
    if img: info['logo'] = img['src']

    c.execute("REPLACE INTO teams VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)",
              (tid, lid, info['name_cn'], info['name_en'], info['founded'], info['country'], info['city'],
               info['stadium'], info['capacity'], info['phone'], info['email'], info['address'], info['logo']))

    # è§£æè£èª‰
    c.execute("DELETE FROM honors WHERE team_id=%s", (tid,))
    honors = []
    if soup.find('div', class_='hornor-record'):
        for h in soup.find('div', class_='hornor-record').find_all('div', class_='hornor-list'):
            raw = h.find('p', class_='header').get_text(strip=True)
            name, cnt = raw.split("X") if "X" in raw else (raw, "1")
            honors.append(
                (tid, name.strip(), cnt.strip(), h.find('span', class_='during-time').get_text(" ", strip=True)))
    if honors: c.executemany("INSERT INTO honors (team_id, name, count, seasons) VALUES (%s,%s,%s,%s)", honors)

    # è§£æé˜µå®¹
    p_ids = re.findall(r'person_id:\s*"(\d+)"', res.text)
    players = []
    if soup.find('div', class_='team-player-data'):
        for i, r in enumerate(soup.find('div', class_='team-player-data').find_all('p', class_='analysis-list-item')):
            pid = p_ids[i] if i < len(p_ids) else ""
            if pid:
                avt = r.find('span', class_='item3').find('img')['src'] if r.find('span', class_='item3').find(
                    'img') else ""
                nat = r.find('span', class_='item6').find('img')['src'] if r.find('span', class_='item6').find(
                    'img') else ""
                players.append((pid, tid, r.find('span', class_='item3').get_text(strip=True),
                                r.find('span', class_='item2').get_text(strip=True),
                                r.find('span', class_='item1').get_text(strip=True), avt, nat))

    if players: c.executemany("REPLACE INTO players VALUES (%s,%s,%s,%s,%s,%s,%s)", players)

    conn.commit()
    conn.close()
    return True


# === æ ¸å¿ƒåŠŸèƒ½ 3ï¼šåˆ·æ–°å•ä¸ªçƒå‘˜ ===
def update_player_data(pid):
    print(f"ğŸ”„ [è‡ªåŠ¨çˆ¬è™«] æ­£åœ¨å®æ—¶æŠ“å–çƒå‘˜: {pid} ...")
    res = fetch(f"https://www.dongqiudi.com/player/{pid}.html")
    if not res: return False
    soup = BeautifulSoup(res.text, 'html.parser')
    conn = get_conn()
    c = conn.cursor()

    prof = {"cn": "", "en": "", "club": "", "nat": "", "h": "", "w": "", "age": "", "birth": "", "num": "", "foot": "",
            "pic": "", "abil": 0, "spd": 0, "sht": 0, "pas": 0, "dri": 0, "def": 0, "pwr": 0}
    left = soup.find('div', class_='info-left')
    if left:
        if left.find('p', class_='china-name'): prof['cn'] = left.find('p', class_='china-name').get_text(strip=True)
        if left.find('p', class_='en-name'): prof['en'] = left.find('p', class_='en-name').get_text(strip=True)
        map_ = {"ä¿±ä¹éƒ¨": "club", "å›½ç±": "nat", "èº«é«˜": "h", "å¹´é¾„": "age", "ä½“é‡": "w", "å·ç ": "num",
                "ç”Ÿæ—¥": "birth", "æƒ¯ç”¨è„š": "foot"}
        for li in left.find_all('li'):
            for k, v in map_.items():
                if k in clean_text(li.get_text()): prof[v] = re.split(r'[:ï¼š]', li.get_text(" ", strip=True))[-1].strip()

    if soup.find('img', class_='player-photo'): prof['pic'] = soup.find('img', class_='player-photo')['src']
    if soup.find('p', class_='average'):
        try:
            prof['abil'] = int(soup.find('p', class_='average').find('b').get_text())
        except:
            pass

    chart = soup.find('div', class_='box_chart')
    if chart:
        amap = {"é€Ÿåº¦": "spd", "å°„é—¨": "sht", "ä¼ çƒ": "pas", "ç›˜å¸¦": "dri", "é˜²å®ˆ": "def", "åŠ›é‡": "pwr"}
        for it in chart.find_all('div', class_='item'):
            txt = it.get_text()
            scr = int(re.search(r'\d+', txt).group()) if re.search(r'\d+', txt) else 0
            for k, v in amap.items():
                if k in txt: prof[v] = scr

    c.execute("REPLACE INTO player_profiles VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)",
              (pid, prof['cn'], prof['en'], prof['club'], prof['nat'], prof['h'], prof['w'], prof['age'], prof['birth'],
               prof['num'], prof['foot'], prof['pic'], prof['abil'], prof['spd'], prof['sht'], prof['pas'], prof['dri'],
               prof['def'], prof['pwr']))

    c.execute("DELETE FROM player_stats WHERE person_id=%s", (pid,))
    stats = []
    wrap = soup.find('div', class_='total-con-wrap')
    if wrap:
        for row in wrap.find_all('p', class_='td'):
            cols = row.find_all('span')
            if len(cols) >= 9:
                try:
                    stats.append((pid, cols[0].get_text(strip=True), cols[1].get_text(strip=True),
                                  int(cols[2].get_text(strip=True)), int(cols[3].get_text(strip=True)),
                                  int(cols[4].get_text(strip=True)), int(cols[5].get_text(strip=True)),
                                  int(cols[6].get_text(strip=True)), int(cols[7].get_text(strip=True))))
                except:
                    continue
    if stats: c.executemany(
        "INSERT INTO player_stats (person_id, season, club, matches, starts, goals, assists, yellow, red) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s)",
        stats)

    conn.commit()
    conn.close()
    return True


def init_db(): pass


def parse_league(name, lid): return []


def parse_team(tid, lid): return []


def parse_player(pid): update_player_data(pid)


if __name__ == "__main__": pass